{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_sentiment_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZlfL9gJR4X5M",
        "HEPu4OvV4aem",
        "7Q8Xkoxwt1jZ",
        "63QkPh8zIdtl"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw2XNfPYaKca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxLcYDfQc-jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjuEmxktdDuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd, numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "print('TF version',tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3ZODxYGdFwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_train():\n",
        "    train=pd.read_csv('/content/drive/My Drive/tweet-sentiment-extraction/train.csv')\n",
        "    train['text']=train['text'].astype(str)\n",
        "    train['selected_text']=train['selected_text'].astype(str)\n",
        "    return train\n",
        "\n",
        "def read_test():\n",
        "    test=pd.read_csv('/content/drive/My Drive/tweet-sentiment-extraction/test.csv')\n",
        "    test['text']=test['text'].astype(str)\n",
        "    return test\n",
        "\n",
        "def read_submission():\n",
        "    test=pd.read_csv('/content/drive/My Drive/tweet-sentiment-extraction/sample_submission.csv')\n",
        "    return test\n",
        "    \n",
        "train_df = read_train()\n",
        "test_df = read_test()\n",
        "submission_df = read_submission()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOBSe40NdHks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str(str1).lower().split()) \n",
        "    b = set(str(str2).lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap13UQHw3gGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_smoothing(labels, factor=0.15):\n",
        "\tlabels *= (1 - factor)\n",
        "\tlabels += (factor / labels.shape[1])\n",
        "\treturn labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlfL9gJR4X5M",
        "colab_type": "text"
      },
      "source": [
        "## Data preproccesing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oknzcz-va4XD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 96\n",
        "PATH = '/content/drive/My Drive/tweet-sentiment-extraction/ROBERT/'\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=PATH+'vocab-roberta-base.json', \n",
        "    merges_file=PATH+'merges-roberta-base.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjbofshD4TwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = train_df.shape[0]\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='float64')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='float64')\n",
        "\n",
        "for k in range(train_df.shape[0]):\n",
        "    \n",
        "    # FIND OVERLAP\n",
        "    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1)   \n",
        "    \n",
        "    # ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "    \n",
        "    # START END TOKENS\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
        "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask[k,:len(enc.ids)+5] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+1] = 1\n",
        "        end_tokens[k,toks[-1]+1] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPu4OvV4aem",
        "colab_type": "text"
      },
      "source": [
        "## Model 00\n",
        "\n",
        "1.   seed=777  \n",
        "\n",
        "2.   cv=0.7104\n",
        "\n",
        "3.   LB=0.713\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iNtlWwD4XWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    ################################################\n",
        "    # HEAD FOR PREDICTION START_TOKEN\n",
        "    ################################################\n",
        "\n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(384, 2, padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Dense(192)(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.LayerNormalization(axis=-1)(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(96, 2, padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Dense(48)(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.LayerNormalization(axis=-1)(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(24, 2, padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    ################################################\n",
        "    # HEAD FOR PREDICTION END_TOKEN\n",
        "    ################################################\n",
        "\n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(384, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Dense(192)(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.LayerNormalization(axis=-1)(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(96, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Dense(48)(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.LayerNormalization(axis=-1)(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(24, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    ################################################\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq-TVqPN4idV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "\n",
        "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 3e-5 * 0.2**x)\n",
        "\n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "        \n",
        "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [label_smoothing(start_tokens[idxT,].copy()), label_smoothing(end_tokens[idxT,].copy())], \n",
        "        epochs=3, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [label_smoothing(start_tokens[idxV,].copy()), label_smoothing(end_tokens[idxV,].copy())]))\n",
        "    \n",
        "    print('Loading model...')\n",
        "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rP_-2484uq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(jac)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q8Xkoxwt1jZ",
        "colab_type": "text"
      },
      "source": [
        "## Model 01\n",
        "\n",
        "1.   seed=444  \n",
        "\n",
        "2.   cv=0.7112\n",
        "\n",
        "3.   LB=0.711"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD5YY234ucB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    ################################################\n",
        "    # HEAD FOR PREDICTION START_TOKEN\n",
        "    ################################################\n",
        "\n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    ################################################\n",
        "    # HEAD FOR PREDICTION END_TOKEN\n",
        "    ################################################\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    ################################################\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kUp3Jt-uAPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jac = []; VER='v01'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=444)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "        \n",
        "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 3e-5 * 0.2**x)\n",
        "\n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "        \n",
        "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [label_smoothing(start_tokens[idxT,].copy()), label_smoothing(end_tokens[idxT,].copy())], \n",
        "        epochs=3, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [label_smoothing(start_tokens[idxV,].copy()), label_smoothing(end_tokens[idxV,].copy())]))\n",
        "    \n",
        "    print('Loading model...')\n",
        "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANtjHyF0lwLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(jac)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63QkPh8zIdtl",
        "colab_type": "text"
      },
      "source": [
        "## Model 02\n",
        "\n",
        "\n",
        "\n",
        "1.   seed=555\n",
        "2.   cv=0.7097\n",
        "3.   PUBLIC LB=0.711\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhlK7y0llwCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "\n",
        "    ################################################\n",
        "    # HEAD FOR PREDICTION START_TOKEN\n",
        "    ################################################\n",
        "    \n",
        "    x1 = tf.keras.layers.SpatialDropout1D(0.1)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    ################################################\n",
        "    # HEAD FOR PREDICTION END_TOKEN\n",
        "    ################################################\n",
        "    \n",
        "    x2 = tf.keras.layers.SpatialDropout1D(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    ################################################\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYzz1nMrl12G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jac = []; VER='v02'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=555)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "        \n",
        "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 3e-5 * 0.2**x)\n",
        "\n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='auto', save_freq='epoch')      \n",
        "\n",
        "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [label_smoothing(start_tokens[idxT,].copy()), label_smoothing(end_tokens[idxT,].copy())], \n",
        "        epochs=3, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [label_smoothing(start_tokens[idxV,].copy()), label_smoothing(end_tokens[idxV,].copy())]))\n",
        "    \n",
        "    print('Loading model...')\n",
        "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anH63asN3f9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(jac)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}